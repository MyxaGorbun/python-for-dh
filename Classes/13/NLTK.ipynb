{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "Это уже третий семинар по собственно NLP, ура! В предыдущих сериях мы занимались частотностью, N-граммами, а также лемматизацией и морфологической разметкой для русского языка. Пришло время для более сложных задач! Кстати, какие задачи NLP вы знаете?\n",
    "\n",
    "Существует несколько хороших NLP-библиотек для питона:\n",
    "* Natural Language Toolkit (NLTK)\n",
    "* Apache OpenNLP\n",
    "* Stanford NLP suite\n",
    "* Gate NLP library\n",
    "\n",
    "Сегодня мы поработаем с NLTK. Устанавливается эта библиотека стандартно, командой `pip install nltk`. Но поскольку в эту библиотеку помимо скриптов входит еще много разных данных -- текстовые корпуса, предобученные модели для анализа тональности и морфологической разметки, списки стоп-слов для разных языков и т.п. -- перед началом работы понадобится скачать нужные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В открывшемся окошке нужно выбрать и скачать следующие пакеты:\n",
    "\n",
    "1. Models\n",
    "    - punkt\n",
    "    - maxent_treebank_pos\n",
    "    - snowball_data\n",
    "    - porter_test\n",
    "    - averaged_perceptron\n",
    "    - averaged_perceptron_russian\n",
    "    - maxent_ne_chunker (необязательно сейчас, но на будущее пригодится)\n",
    "2. Corpora\n",
    "    - movie_reviews\n",
    "    - stopwords\n",
    "    - brown (на будущее)\n",
    "3. All Packages\n",
    "    - wordnet\n",
    "    - universal_tagset\n",
    "    \n",
    "Можно скачать и все данные сразу (`Collections > all`), но они будут качаться долго и займут много места. И среди них есть вещи, которые скорее всего вам не понадобятся -- например, баскский корпус.\n",
    "\n",
    "Мы посмотрим лишь на базовые функции NLTK и разберем один сложный пример. Для дальнейшей работы [вот тут](https://www.nltk.org/book/) можно найти учебник по NLTK от его авторов, а [вот тут](https://github.com/hb20007/hands-on-nltk-tutorial) много тьюториалов, где показывается, как решать различные задачи с помощью инструментов NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Военно-морская любовь\n",
    "\n",
    "По морям, играя, носится\n",
    "с миноносцем миноносица.\n",
    "\n",
    "Льнет, как будто к меду осочка,\n",
    "к миноносцу миноносочка.\n",
    "\n",
    "И конца б не довелось ему,\n",
    "благодушью миноносьему.\n",
    "\n",
    "Вдруг прожектор, вздев на нос очки,\n",
    "впился в спину миноносочки.\n",
    "\n",
    "Как взревет медноголосина:\n",
    "\"Р-р-р-астакая миноносина!\"\n",
    "\n",
    "Прямо ль, влево ль, вправо ль бросится,\n",
    "а сбежала миноносица.\n",
    "\n",
    "Но ударить удалось ему\n",
    "по ребру по миноносьему.\n",
    "\n",
    "Плач и вой морями носится:\n",
    "овдовела миноносица.\n",
    "\n",
    "И чего это несносен нам\n",
    "мир в семействе миноносином?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Военно-морская', 'любовь', 'По', 'морям', ',', 'играя', ',', 'носится', 'с', 'миноносцем', 'миноносица', '.', 'Льнет', ',', 'как', 'будто', 'к', 'меду', 'осочка', ',', 'к', 'миноносцу', 'миноносочка', '.', 'И', 'конца', 'б', 'не', 'довелось', 'ему', ',', 'благодушью', 'миноносьему', '.', 'Вдруг', 'прожектор', ',', 'вздев', 'на', 'нос', 'очки', ',', 'впился', 'в', 'спину', 'миноносочки', '.', 'Как', 'взревет', 'медноголосина', ':', \"''\", 'Р-р-р-астакая', 'миноносина', '!', \"''\", 'Прямо', 'ль', ',', 'влево', 'ль', ',', 'вправо', 'ль', 'бросится', ',', 'а', 'сбежала', 'миноносица', '.', 'Но', 'ударить', 'удалось', 'ему', 'по', 'ребру', 'по', 'миноносьему', '.', 'Плач', 'и', 'вой', 'морями', 'носится', ':', 'овдовела', 'миноносица', '.', 'И', 'чего', 'это', 'несносен', 'нам', 'мир', 'в', 'семействе', 'миноносином', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сплиттинг\n",
    "\n",
    "Этим красивым словом называется разбиение текста на предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nВоенно-морская любовь\\n\\nПо морям, играя, носится\\nс миноносцем миноносица.',\n",
       " 'Льнет, как будто к меду осочка,\\nк миноносцу миноносочка.',\n",
       " 'И конца б не довелось ему,\\nблагодушью миноносьему.',\n",
       " 'Вдруг прожектор, вздев на нос очки,\\nвпился в спину миноносочки.',\n",
       " 'Как взревет медноголосина:\\n\"Р-р-р-астакая миноносина!\"',\n",
       " 'Прямо ль, влево ль, вправо ль бросится,\\nа сбежала миноносица.',\n",
       " 'Но ударить удалось ему\\nпо ребру по миноносьему.',\n",
       " 'Плач и вой морями носится:\\nовдовела миноносица.',\n",
       " 'И чего это несносен нам\\nмир в семействе миноносином?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Военно-морская', 'любовь', 'По', 'морям', ',', 'играя', ',', 'носится', 'миноносцем', 'миноносица', '.', 'Льнет', ',', 'меду', 'осочка', ',', 'миноносцу', 'миноносочка', '.', 'И', 'конца', 'б', 'довелось', ',', 'благодушью', 'миноносьему', '.', 'Вдруг', 'прожектор', ',', 'вздев', 'нос', 'очки', ',', 'впился', 'спину', 'миноносочки', '.', 'Как', 'взревет', 'медноголосина', ':', \"''\", 'Р-р-р-астакая', 'миноносина', '!', \"''\", 'Прямо', 'ль', ',', 'влево', 'ль', ',', 'вправо', 'ль', 'бросится', ',', 'сбежала', 'миноносица', '.', 'Но', 'ударить', 'удалось', 'ребру', 'миноносьему', '.', 'Плач', 'вой', 'морями', 'носится', ':', 'овдовела', 'миноносица', '.', 'И', 'это', 'несносен', 'нам', 'мир', 'семействе', 'миноносином', '?']\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('russian')\n",
    "\n",
    "# какие слова исчезли?\n",
    "filtered = [w for w in word_tokenize(text) if w not in sw]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Военно-морская', 'любовь'),\n",
       " ('любовь', 'По'),\n",
       " ('По', 'морям'),\n",
       " ('морям', ','),\n",
       " (',', 'играя'),\n",
       " ('играя', ','),\n",
       " (',', 'носится'),\n",
       " ('носится', 'с'),\n",
       " ('с', 'миноносцем'),\n",
       " ('миноносцем', 'миноносица'),\n",
       " ('миноносица', '.'),\n",
       " ('.', 'Льнет'),\n",
       " ('Льнет', ','),\n",
       " (',', 'как'),\n",
       " ('как', 'будто'),\n",
       " ('будто', 'к'),\n",
       " ('к', 'меду'),\n",
       " ('меду', 'осочка'),\n",
       " ('осочка', ','),\n",
       " (',', 'к'),\n",
       " ('к', 'миноносцу'),\n",
       " ('миноносцу', 'миноносочка'),\n",
       " ('миноносочка', '.'),\n",
       " ('.', 'И'),\n",
       " ('И', 'конца'),\n",
       " ('конца', 'б'),\n",
       " ('б', 'не'),\n",
       " ('не', 'довелось'),\n",
       " ('довелось', 'ему'),\n",
       " ('ему', ','),\n",
       " (',', 'благодушью'),\n",
       " ('благодушью', 'миноносьему'),\n",
       " ('миноносьему', '.'),\n",
       " ('.', 'Вдруг'),\n",
       " ('Вдруг', 'прожектор'),\n",
       " ('прожектор', ','),\n",
       " (',', 'вздев'),\n",
       " ('вздев', 'на'),\n",
       " ('на', 'нос'),\n",
       " ('нос', 'очки'),\n",
       " ('очки', ','),\n",
       " (',', 'впился'),\n",
       " ('впился', 'в'),\n",
       " ('в', 'спину'),\n",
       " ('спину', 'миноносочки'),\n",
       " ('миноносочки', '.'),\n",
       " ('.', 'Как'),\n",
       " ('Как', 'взревет'),\n",
       " ('взревет', 'медноголосина'),\n",
       " ('медноголосина', ':'),\n",
       " (':', \"''\"),\n",
       " (\"''\", 'Р-р-р-астакая'),\n",
       " ('Р-р-р-астакая', 'миноносина'),\n",
       " ('миноносина', '!'),\n",
       " ('!', \"''\"),\n",
       " (\"''\", 'Прямо'),\n",
       " ('Прямо', 'ль'),\n",
       " ('ль', ','),\n",
       " (',', 'влево'),\n",
       " ('влево', 'ль'),\n",
       " ('ль', ','),\n",
       " (',', 'вправо'),\n",
       " ('вправо', 'ль'),\n",
       " ('ль', 'бросится'),\n",
       " ('бросится', ','),\n",
       " (',', 'а'),\n",
       " ('а', 'сбежала'),\n",
       " ('сбежала', 'миноносица'),\n",
       " ('миноносица', '.'),\n",
       " ('.', 'Но'),\n",
       " ('Но', 'ударить'),\n",
       " ('ударить', 'удалось'),\n",
       " ('удалось', 'ему'),\n",
       " ('ему', 'по'),\n",
       " ('по', 'ребру'),\n",
       " ('ребру', 'по'),\n",
       " ('по', 'миноносьему'),\n",
       " ('миноносьему', '.'),\n",
       " ('.', 'Плач'),\n",
       " ('Плач', 'и'),\n",
       " ('и', 'вой'),\n",
       " ('вой', 'морями'),\n",
       " ('морями', 'носится'),\n",
       " ('носится', ':'),\n",
       " (':', 'овдовела'),\n",
       " ('овдовела', 'миноносица'),\n",
       " ('миноносица', '.'),\n",
       " ('.', 'И'),\n",
       " ('И', 'чего'),\n",
       " ('чего', 'это'),\n",
       " ('это', 'несносен'),\n",
       " ('несносен', 'нам'),\n",
       " ('нам', 'мир'),\n",
       " ('мир', 'в'),\n",
       " ('в', 'семействе'),\n",
       " ('семействе', 'миноносином'),\n",
       " ('миноносином', '?')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b for b in nltk.bigrams(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object trigrams at 0x0000018C36F6F468>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.trigrams(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг\n",
    "\n",
    "#### Стеммер Портера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'d: 'd\n",
      "'s: 's\n",
      "'t: 't\n",
      ",: ,\n",
      ".: .\n",
      ":: :\n",
      ";: ;\n",
      "And: and\n",
      "Devoutly: devoutli\n",
      "For: for\n",
      "Must: must\n",
      "No: No\n",
      "Or: Or\n",
      "That: that\n",
      "The: the\n",
      "To: To\n",
      "When: when\n",
      "Whether: whether\n",
      "a: a\n",
      "against: against\n",
      "and: and\n",
      "arms: arm\n",
      "arrows: arrow\n",
      "be: be\n",
      "by: by\n",
      "calamity: calam\n",
      "coil: coil\n",
      "come: come\n",
      "consummation: consumm\n",
      "death: death\n",
      "die: die\n",
      "die—to: die—to\n",
      "dreams: dream\n",
      "dream—ay: dream—ay\n",
      "end: end\n",
      "flesh: flesh\n",
      "fortune: fortun\n",
      "give: give\n",
      "have: have\n",
      "heart-ache: heart-ach\n",
      "heir: heir\n",
      "in: in\n",
      "is: is\n",
      "life: life\n",
      "long: long\n",
      "makes: make\n",
      "may: may\n",
      "mind: mind\n",
      "more: more\n",
      "mortal: mortal\n",
      "natural: natur\n",
      "nobler: nobler\n",
      "not: not\n",
      "of: of\n",
      "off: off\n",
      "opposing: oppos\n",
      "or: or\n",
      "outrageous: outrag\n",
      "pause—there: pause—ther\n",
      "perchance: perchanc\n",
      "question: question\n",
      "respect: respect\n",
      "rub: rub\n",
      "say: say\n",
      "sea: sea\n",
      "shocks: shock\n",
      "shuffled: shuffl\n",
      "sleep: sleep\n",
      "slings: sling\n",
      "so: so\n",
      "suffer: suffer\n",
      "take: take\n",
      "that: that\n",
      "the: the\n",
      "them: them\n",
      "there: there\n",
      "this: thi\n",
      "thousand: thousand\n",
      "to: to\n",
      "troubles: troubl\n",
      "us: us\n",
      "we: we\n",
      "what: what\n",
      "wish: wish\n"
     ]
    }
   ],
   "source": [
    "# неловкий момент: работает только для английского\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "hamlet = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To die—to sleep,\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to: 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep, perchance to dream—ay, there's the rub:\n",
    "For in that sleep of death what dreams may come,\n",
    "When we have shuffled off this mortal coil,\n",
    "Must give us pause—there's the respect\n",
    "That makes calamity of so long life.\n",
    "\"\"\"\n",
    "\n",
    "porter = PorterStemmer()\n",
    "words = set(word_tokenize(hamlet))\n",
    "\n",
    "for w in sorted(words):\n",
    "    print(\"%s: %s\" % (w, porter.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# улучшенный вараинт стеммера Портера, умеет разбирать не только английский\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: !\n",
      "'': '\n",
      ",: ,\n",
      ".: .\n",
      ":: :\n",
      "?: ?\n",
      "Вдруг: вдруг\n",
      "Военно-морская: военно-морск\n",
      "И: и\n",
      "Как: как\n",
      "Льнет: льнет\n",
      "Но: но\n",
      "Плач: плач\n",
      "По: по\n",
      "Прямо: прям\n",
      "Р-р-р-астакая: р-р-р-астак\n",
      "а: а\n",
      "б: б\n",
      "благодушью: благодуш\n",
      "бросится: брос\n",
      "будто: будт\n",
      "в: в\n",
      "вздев: вздев\n",
      "взревет: взревет\n",
      "влево: влев\n",
      "вой: во\n",
      "впился: впил\n",
      "вправо: вправ\n",
      "довелось: довел\n",
      "ему: ем\n",
      "и: и\n",
      "играя: игр\n",
      "к: к\n",
      "как: как\n",
      "конца: конц\n",
      "ль: л\n",
      "любовь: любов\n",
      "медноголосина: медноголосин\n",
      "меду: мед\n",
      "миноносина: миноносин\n",
      "миноносином: миноносин\n",
      "миноносица: миноносиц\n",
      "миноносочка: миноносочк\n",
      "миноносочки: миноносочк\n",
      "миноносцем: миноносц\n",
      "миноносцу: миноносц\n",
      "миноносьему: минонос\n",
      "мир: мир\n",
      "морям: мор\n",
      "морями: мор\n",
      "на: на\n",
      "нам: нам\n",
      "не: не\n",
      "несносен: неснос\n",
      "нос: нос\n",
      "носится: нос\n",
      "овдовела: овдовел\n",
      "осочка: осочк\n",
      "очки: очк\n",
      "по: по\n",
      "прожектор: прожектор\n",
      "ребру: ребр\n",
      "с: с\n",
      "сбежала: сбежа\n",
      "семействе: семейств\n",
      "спину: спин\n",
      "удалось: уда\n",
      "ударить: удар\n",
      "чего: чег\n",
      "это: эт\n"
     ]
    }
   ],
   "source": [
    "ruswords = set(word_tokenize(text))\n",
    "\n",
    "for w in sorted(ruswords):\n",
    "    print(\"%s: %s\" % (w, snowball.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация и POS-tagging\n",
    "\n",
    "Прямо скажем, это не самая сильная сторона NLTK.  Для этих задач лучше использовать `pymorphy2` и `pymystem3` для русского языка и `Spacy` для европейских.\n",
    "\n",
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-tagging\n",
    "\n",
    "#### UPenn Tagset\n",
    "\n",
    "Теггер в NLTK по умолчанию использует своеобразную систему тегов (очень и очень устаревшую), которая, в числе прочего, описана [вот здесь](https://www.nltk.org/book/ch05.html). Она называется _UPenn Tagset_ -- по английскому корпусу Penn Treebank, где она использовалась.\n",
    "\n",
    "* CC -- coordinating conjunction\n",
    "* CD -- cardinal digit\n",
    "* DT -- determiner\n",
    "* EX -- existential there (“there is”, “there exists”)\n",
    "* FW -- foreign word\n",
    "* IN -- preposition/subordinating conjunction\n",
    "* JJ -- adjective (‘big’)\n",
    "* JJR -- adjective, comparative (‘bigger’)\n",
    "* JJS -- adjective, superlative (‘biggest’)\n",
    "* LS -- list marker\n",
    "* MD -- modal ('could', 'will')\n",
    "* NN -- noun, singular (‘desk’)\n",
    "* NNS -- noun plural (‘desks’)\n",
    "* NNP -- proper noun, singular (‘Harrison’)\n",
    "* NNPS -- proper noun, plural (‘Americans’)\n",
    "* PDT -- predeterminer (‘all the kids’)\n",
    "* POS -- possessive ending ('parent’s')\n",
    "* PRP -- personal pronoun ('I', 'he', 'she')\n",
    "* PRPS -- possessive pronoun ('my', 'his', 'hers')\n",
    "* RB -- adverb ('very', 'silently')\n",
    "* RBR -- adverb, comparative ('better')\n",
    "* RBS -- adverb, superlative ('best')\n",
    "* RP -- particle ('give up')\n",
    "* TO -- to-particle ('to go') \n",
    "* UH -- interjection ('errrrrrrrm')\n",
    "* VB -- verb, base form ('take')\n",
    "* VBD -- verb, past tense ('took')\n",
    "* VBG -- verb, gerund/present participle ('taking')\n",
    "* VBN -- verb, past participle ('taken')\n",
    "* VBP -- verb, sing. present, non-3d ('take')\n",
    "* VBZ -- verb, 3rd person sing. present ('takes')\n",
    "* WDT -- wh-determiner ('which')\n",
    "* WP -- wh-pronoun ('who', 'what')\n",
    "* WP -- possessive wh-pronoun ('whose')\n",
    "* WRB -- wh-abverb ('where', 'when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NNP'),\n",
       " ('there', 'EX'),\n",
       " (\"I'm\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('POS-tagger', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"Hey there I'm a POS-tagger\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal Dependencies\n",
    "\n",
    "Но -- ура! -- теперь подерживает и тегсет из _Universal Dependencies_, и **лучше использовать его**. Это нужно указать с помощью специального параметра `tagset` (предварительно скачав). Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги -- [вот тут](http://universaldependencies.org/u/pos/). Вот список основных тегов UD:\n",
    "\n",
    "* ADJ: adjective\n",
    "* ADP: adposition\n",
    "* ADV: adverb\n",
    "* AUX: auxiliary\n",
    "* CCONJ: coordinating conjunction\n",
    "* DET: determiner\n",
    "* INTJ: interjection\n",
    "* NOUN: noun\n",
    "* NUM: numeral\n",
    "* PART: particle\n",
    "* PRON: pronoun\n",
    "* PROPN: proper noun\n",
    "* PUNCT: punctuation\n",
    "* SCONJ: subordinating conjunction\n",
    "* SYM: symbol\n",
    "* VERB: verb\n",
    "* X: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NOUN'),\n",
       " ('there', 'DET'),\n",
       " (\"I'm\", 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('POS-tagger', 'NOUN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"Hey there I'm a POS-tagger\".split(), tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "1. Скачайте файл с английским текстом \"Алисы в Зазеркалье\" из папки с семинаром.\n",
    "2. Очистите его от пунктуации и приведите к нижнему регистру\n",
    "3. Разбейте на биграммы и на триграммы с помощью NLTK. Выведите по 20 самых частотных биграмм и триграмм.\n",
    "4. Сделайте частеречную разметку текста и запишите в новый файл так, чтобы на каждой строке было одно слово и его тег.\n",
    "4. Лемматизируйте текст и запишите в новый файл.\n",
    "5. Очистите лемматизированный текст от стоп-слов и составьте частотный список слов. Выведите 30 самых частотных.\n",
    "6. Посчитайте ipm слов 'alice', 'unicorn', 'tweedledum', 'tweedledee', 'walrus', 'looking-glass'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация документов\n",
    "\n",
    "Умеет ли NLTK что-то посложнее выделения N-грамм и стемминга? Да! Посмотрим пример классификации документов, т.е. разбиение некоторого корпуса текстов на классы на основе различных признаков (`features`, в простонародье \"фичи\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем один из готовых корпусов: отзывы на фильмы\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]   # как называется вся эта конструкция в [ ]?\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно выбрать фичи, т.е. решить, на основании чего мы будем классифицировать тексты. Это называется красивым термином `feature engineering`. Например, мы можем выбрать в качестве фичей сами слова, т.е. наш классификатор будет принимать решение о том, к какому классу отнести тот или иной текст, на основании слов, которые в нем встречаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# смотрим частотное распределение слов\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "\n",
    "# обрезаем низкочастотный хвост: берем только 2000 самых частотных слов из всех\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# функция, которая извлекает фичи из документа\n",
    "def document_features(document): \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно создать и обучить классификатор (для примера возьмем наивный байесовский классификатор). Подробнее о классификации, кластеризации и прочих увлекательных вещах можно узнать, углубившись в тему \"машинное обучение\" (осторожно: математика!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сопоставляем фичи и классы\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# разбиваем корпус на тренировочную и тестовую выборку\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# обучаем классификатор\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь классифицируем тестовую выборку и оцениваем качество работы классификатора с помощью подсчета accuracy (\"точность\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "Most Informative Features\n",
      " contains(unimaginative) = True              neg : pos    =      8.4 : 1.0\n",
      "        contains(welles) = True              neg : pos    =      7.7 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.0 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.8 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      6.6 : 1.0\n",
      "       contains(jumbled) = True              neg : pos    =      6.4 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      6.4 : 1.0\n",
      "       contains(singers) = True              pos : neg    =      6.3 : 1.0\n",
      "           contains(ugh) = True              neg : pos    =      5.8 : 1.0\n",
      "         contains(waste) = True              neg : pos    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# оцениваем качество\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# смотрим 10 самых информативных фичей\n",
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
